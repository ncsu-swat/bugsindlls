\documentclass[sigconf,screen]{acmart}

\usepackage{multirow}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{listings}
\usepackage{enumitem}
\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  frame=single,
  breaklines=true,
  postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation email}{June 03--05,
  2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/2018/06}
\acmBooktitle{Companion Proceedings of the 33rd ACM Symposium on the Foundations
of Software Engineering (FSE '25), June 23--27, 2025, Trondheim, Norway}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

\input{macros}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{\tname: A Database of Reproducible Bugs in Deep
  Learning Libraries to Enable Systematic Evaluation of Testing
  Techniques}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
%% \author{Ben Trovato}
%% \authornote{Both authors contributed equally to this research.}
%% \email{trovato@corporation.com}
%% \orcid{1234-5678-9012}
%% \author{G.K.M. Tobin}
%% \authornotemark[1]
%% \email{webmaster@marysville-ohio.com}
%% \affiliation{%
%%   \institution{Institute for Clarity in Documentation}
%%   \city{Dublin}
%%   \state{Ohio}
%%   \country{USA}
%% }

%% \author{Lars Th{\o}rv{\"a}ld}
%% \affiliation{%
%%   \institution{The Th{\o}rv{\"a}ld Group}
%%   \city{Hekla}
%%   \country{Iceland}}
%% \email{larst@affiliation.org}

\author{M. M. Abid Naziri}
\affiliation{%
  \institution{NC State University}
  %\city{Rocquencourt}
  \country{USA}
}
\email{mnaziri@ncsu.edu}

\author{Aman Kumar Singh}
\affiliation{%
 \institution{Amrita Vishwa Vidyapeetham}
 %% \city{Doimukh}
 %% \state{Arunachal Pradesh}
 \country{India}}
\email{amanks@am.amrita.edu}

\author{Feiran Qin}
\affiliation{%
 \institution{NC State University}
 %% \city{Doimukh}
 %% \state{Arunachal Pradesh}
 \country{USA}}
\email{fqin2@ncsu.edu}

\author{Benjamin Wu}
\affiliation{%
 \institution{Purdue University}
 %% \city{Doimukh}
 %% \state{Arunachal Pradesh}
 \country{USA}}
\email{wu2059@purdue.edu}

\author{Saikat Dutta}
\affiliation{%
 \institution{Cornell University}
 %% \city{Doimukh}
 %% \state{Arunachal Pradesh}
 \country{USA}}
\email{saikatd@cornell.edu}

\author{Marcelo d'Amorim}
\affiliation{%
 \institution{NC State University}
 %% \city{Doimukh}
 %% \state{Arunachal Pradesh}
 \country{USA}}
\email{mdamori@ncsu.edu}


%% \author{Julius P. Kumquat}
%% \affiliation{%
%%   \institution{The Kumquat Consortium}
%%   \city{New York}
%%   \country{USA}}
%% \email{jpkumquat@consortium.net}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Trovato et al.}


%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  \sloppy
  AI-enabled applications are prolific today. Deep Learning~(DL)
  libraries, such as \torch{} and \tf{}, provide the building
  blocks for the AI components of these applications. As any piece of
  software, these libraries can be buggy.
  %% and those bugs can affect a
  %% great deal of applications using those libraries.
  An impressive number of bug-finding techniques to address this
  problem have been proposed, but the lack of a curated set of
  reproducible bugs in DL libraries hinders credible evaluation of these
  techniques. We present \tname, a database of curated reproducible
  bugs to fill that gap.
  %% enable credible  evaluation of bug-finding DL library testing techniques.
  Our dataset currently consists of \numbugs{} environments to
  reproduce bugs across three popular DL libraries, namely, \jax,
  \tf, and \torch.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
%% \begin{CCSXML}
%% <ccs2012>
%%  <concept>
%%   <concept_id>00000000.0000000.0000000</concept_id>
%%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%%   <concept_significance>500</concept_significance>
%%  </concept>
%%  <concept>
%%   <concept_id>00000000.00000000.00000000</concept_id>
%%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%%   <concept_significance>300</concept_significance>
%%  </concept>
%%  <concept>
%%   <concept_id>00000000.00000000.00000000</concept_id>
%%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%%   <concept_significance>100</concept_significance>
%%  </concept>
%%  <concept>
%%   <concept_id>00000000.00000000.00000000</concept_id>
%%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%%   <concept_significance>100</concept_significance>
%%  </concept>
%% </ccs2012>
%% \end{CCSXML}

%% \ccsdesc[500]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
%% \ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
%% \ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
%% \ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Deep learning libraries, testing, benchmarking}
%% %% A "teaser" image appears between the author and affiliation
%% %% information and the body of the document, and typically spans the
%% %% page.
%% \begin{teaserfigure}
%%   \includegraphics[width=\textwidth]{sampleteaser}
%%   \caption{Seattle Mariners at Spring Training, 2010.}
%%   \Description{Enjoying the baseball game from the third-base
%%   seats. Ichiro Suzuki preparing to bat.}
%%   \label{fig:teaser}
%% \end{teaserfigure}

%% \received{20 February 2007}
%% \received[revised]{12 March 2009}
%% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

\sloppy Serveral important application domains (e.g., transportation,
medicine, etc.) use AI as part of their business processes. Deep
Learning~(DL) libraries, such as \jax, \torch{}, and \tf{}, provide
the building blocks for the AI components of these applications.
Unfortunately, as any piece of software, these libraries contain
bugs. An impressive number of techniques have been recently proposed
to find bugs in these
libraries~\cite{wei2022free,xie2022docter,deng2022fuzzing,pham2019cradle,guo2020audee,wang2020deep,gu2022muffin,deng2023large,liu2023nnsmith,liu2023neuri,shi2023acetest,deng2023largeedge},
however we observe that the evaluation of these techniques do
\emph{not} use a reference set of reproducible bugs to evaluate their
effectiveness. The lack of evaluation standards is an obstacle to the
fair comparison of techniques, hindering research progress.

We present \tname, a database of curated reproducible bugs to
enable credible evaluation of DL library testing techniques. \tname\ is
equipped with a command-line interface to enable researchers to analyze
and reproduce bug instances. The following screencast demonstrate some
of the features in \tname:

\url{https://tinyurl.com/BugsInDLLs}\\

\tname\ has been under active development since March 21, 2024, the
day of the first commit in the corresponding GitHub repository. Two
UROP students, two PhD students, and two faculty were involved in the
work. \tname\ is publicly available from the following URL:

\url{https://github.com/ncsu-swat/bugsindlls}

\section{Objects and Methods}

\begin{table}
  \centering
  \caption{\label{table:bug-characterization}Characterization of
    reproducible bugs from \tname.\Fix{Is this up to date? I have seen
  lots of activity recently but these numbers remain the
  same - Fixed (Abid)}\Fix{Can we add info about whether Python and/or C code causes
      the bug? - Abid: we report how many c,python and cuda native files are buggy in total but not per bug, so currently no}\Fix{Can we add info on whether it is GPU related
      (i.e., only manifests in GPUs)? - Abid: venv-docker is the same as cpu-gpu}}
\begin{tabular}{l|rrr}
  \toprule & \multirow{2}{*}{\#} & \multicolumn{1}{r}{build} &
  \multicolumn{1}{r}{enviroment} \\ & & release-nightly & venv-docker
  \\ \cmidrule(lr){2-4} \jax{} & 46 & 45-1 & 45-1 \\ \torch{} & 37 &
  18-19 & 21-16 \\ \tf{} & 24 & 23-1 & 24-0 \\ \midrule
  \multicolumn{1}{c|}{$\Sigma$} & \numbugs{} & 86-21& 90-17
  \\ \bottomrule
\end{tabular}
\end{table}

This section describes the criteria for selecting libraries and bugs
(Section~\ref{sec:selection-criterion}), the challenges for bug
reproduction (Section~\ref{sec:challenges}), and the method we
followed to create bug instances (Section~\ref{sec:method}).

\tname\ contains \numbugs{} instances of reproducible bugs
representing three popular DL libraries, namely, \jax, \tf, and
\torch. Table~\ref{table:bug-characterization} shows the list of bug
instances for each supported library. Column ``\#'' shows the number
of bug instances, column ``build'' shows the breakdown of instances
for each kind of build, and column ``environment'' shows the breakdown
of instances for each kind of environment to reproduce the bug.

\subsection{Selection Criteria}
\label{sec:selection-criterion}

% \Fix{Abid, explain the rationale for selecting jax, torch, and tf.}

\textbf{Libraries.}~\torch~and \tf are popular DL libraries
intensively used in the literature. \jax\ is a newer library that has
been rapidly gaining popularity. All these libraries are open source
with active communities supporting their maintenance.

%% We chose these three popular datasets in order to enhance the process
%% of developing testing tools around these libraries.

% \Fix{Abid, explain the criterion for selecting the bugs (e.g., it
  % contains test code showing the problem).}

\textbf{Bugs.}~We focused on bug reports that have been accepted by
the developer community as true bugs. We determine this by checking
whether the bugs have appropriate labels and have been fixed with pull
requests linked to them. Among these confirmed bug reports, we choose
the ones that have enough information to reproduce the bugs, i.e.,
code to reproduce the bug, dependencies required for the bug
reproduction
% \Fix{Abid, can you confirm that environment here is not
%   the same as venv-docker that we are using on Table 1? instead it
%   means the version of CUDA, libraries, etc.? If so, avoid (re)using the
%   word environment.}
, clear description of the desired behavior,
etc. Once we can successfully reproduce the bug by creating the
enviroment detailed in the report, we choose the bug to be included in
our dataset.

\subsection{Challenges}
\label{sec:challenges}

\textbf{Handling bugs from nighly builds requires saving wheels.}
Many of the reproducible bugs rely on the libraries' nightly builds,
which are available for a limited amount of time. To ensure that bugs
reported on these builds remain reproducible, it is necessary to save
Python wheel files (.whl)~\cite{wheels} for the correspondind builds.

\textbf{Handling bugs in specific GPU-CUDA versions requires OS
  changes:} Some GPU-related bug instances can only be reproduced with
specific versions of CUDA~\cite{cuda}, NVIDIA's platform and API for
programming GPUs. These instances require specific CUDA drivers to be
installed on the system. Python virtual enviroments are not designed
to enable change of OS drivers.
% GPU builds\Fix{I don't understance GPU builds. do you mean
%   GPU vendor/version? give example} and specific CUDA builds. 
% Changing different virtual environments does not change the CUDA
% versions\Fix{Do you mean ``it is not possible to change the CUDA (or
%   GPU) version with Python's virtual environments''?} of the
% system. 
% That is why we use Docker when a bug relies on a specific
% version of CUDA to be installed in the system.
We use Docker containers~\cite{merkel2014docker} for the bug instances
that require changes in drivers.

\subsection{Method}
\label{sec:method}

The following steps show the method we use to create bug instances:

\begin{enumerate}[leftmargin=0.5cm]
  \item Select an issue following the criteria defined in Section~\ref{sec:selection-criterion};
  \item Write file \CodeIn{requirements.txt} with the list of dependencies to reproduce the bug;
  \item Create a file showing the code that causes the bug (for documentation);
  \item Create a file with a \CodeIn{pytest} test case that passes if
    the code triggers the buggy behavior documented in the issue;
    %% A python file is created that contains the bug revealing code, along with a test (pytest) that passes upon successfully trigerring the buggy behavior.
  \item If the bug depends on a CUDA-specific build, write a
    Dockerfile to create a container including the needed CUDA-driver
    and the other artifacts mentioned above (e.g., test file and
    requirements);
    %% along with the requirements.txt file and the python
    %% script copied into it.
  \item Write a script that creates the virtual environment (either
    Python's \CodeIn{Conda Enviroment}~\cite{venv} or Docker container) and runs
    the test on it;
  \item Create a directory \CodeIn{<lib>/<bug-id>} containing all the
    artifacts mentioned above, where \CodeIn{lib} refers to the
    affected library and \CodeIn{bug-id} is a fresh identifier of the
    bug instance.
    %% All files are put into a single self-contained folder with the issue id in the name so that the tools exposed by the framework of our tool can find and execute the the bug reproduction script.
\end{enumerate}

\textbf{Test Oracles.}~
The bug reproduction code includes python tests (pytest). The test
oracle is satisfied when the bug is reproduced successfully. For bugs
that throw an unexpected exception, the test catches the exception and
reports the exception details along with a pass status. For bugs that
result in incorrect output value, assertions are placed in the code to
expect the incorrect output value, hence passing the test when the bug
occurs. For bugs that crash after raising a signal, the code snippet
is placed in a seperate file and executed, while the file contaning
the actual test asserts the presence of the signal raised by the
code. Upon successful reproduction, the original code crashes raising
the proper signal and the assertion passes.\Fix{Please review and
  revise}

\textbf{Example}
% ~\Fix{Can you show here the artifacts associated to bug
%   120903 that you will use in the next section? If it takes too much
%   space, show only directory with files and the test case. - Abid: by the test case, do you mean the python code used to reproduce the bug?} 
  The issue no. 120903 in pytorch has the following test code that checks the oracle:

\begin{lstlisting}[language=python]
def test_f():
  input_data = torch.randn(3, 4, 5, 6)
  scale = np.array([0.1, 0.2, 0.3, 0.4])
  zero_point = torch.tensor([1, 2, 3, 4], dtype=torch.int32)
  axis = 1
  quant_min = 0
  quant_max = 255
  with pytest.raises(RuntimeError) as e_info:
      output = torch.fake_quantize_per_channel_affine(input_data, torch.from_numpy(scale), zero_point, axis, quant_min, quant_max)
  print(f'{e_info.type.__name__}: {e_info.value}')
\end{lstlisting}

In this example, the unexpected behavior is a \CodeIn{RuntimeError} thrown by the call to the \CodeIn{fake\_quantize\_per\_channel\_affine} API from \torch. Since the test oracle should pass when the bug is successfully reproduced, the code catches the exception and prints information about it, and upon catching the exception successfully, the test passes. If the bug reproduction fails i.e. the exception is not thrown or a different exception is thrown, the test will fail.

\section{\tname} % : Database of Bugs in DL libraries

This section presents the interface of \tname\ and walks the reader
through a demonstration showing the tool's functionality.


\subsection{Interface}

\begin{table}
  \centering
  \caption{\label{table:bug-interface}\tname's bug interface.}
\begin{tabular}{lp{6.5cm}}
  \toprule
  \textbf{command} & \textbf{description} \\
  \midrule
list-tests & List the tests available on this dataset\\
run-test & Runs one test\\
run-tests & Runs all the tests\\
run-tool & Runs a testing tool in a given buggy environment \\
show-info & Shows information about the tests available on this benchmark\\
stats & Shows statistics about this dataset (e.g., number of tests
that require GPU, number of tests that reproduce bugs in C or Python
code, etc.)\\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{table:bug-interface} shows the list of commands in the
\tname's interface along with a short description for these
commands. In the following we demonstrate \tname.
% \Fix{explain that the test passes if the bug can be reproduced}

\subsection{Usage}

To enable system-wide access of the framework, it is necessary to add
the directory \CodeIn{/framework} to the \CodeIn{PATH} environment
variable. Run the following commands for that:

\begin{lstlisting}[language=bash]
$> git clone git@github.com:ncsu-swat/bugsindlls.git
$> cd bugsindlls
$> export PATH=$PATH:`pwd`/framework
\end{lstlisting}


\subsubsection{Running one bug instance}

% \Fix{walkthrough an example. Show input and part of output. Explain
%   the output.}


Let us use the bug 120903~\Fix{cite} from \torch\ to demonstrate the
command \CodeIn{run-test}, which runs a test to reproduce a bug on an
environment with the necessary dependencies installed. Use the
following command to reproduce the bug:

\begin{lstlisting}[language=bash]
$> run-test --library-name pytorch --bug-id 120903
\end{lstlisting}

\noindent
Execution of this command produces the following output:

\begin{lstlisting}[language=bash]
... 
Versions of relevant libraries:
[pip3] numpy==1.26.4
[pip3] torch==2.2.0+cpu
[conda] torch                     2.2.0+cpu                pypi_0    pypi
====== test session starts ======
platform linux -- Python 3.10.0, pytest-8.2.0, pluggy-1.5.0
rootdir: /home/mnaziri/Documents/DL_Testing/dnnbugs/pytorch/issue_120903
collected 1 item                                                                            

test_issue_120903.py Pytorch issue no. 120903
Seed:  120903
RuntimeError: !needs_dynamic_casting<func_t>::check(iter) INTERNAL ASSERT FAILED at "../aten/src/ATen/native/cpu/Loops.h":310, please report a bug to PyTorch. 
====== 1 passed in 0.96s ======
\end{lstlisting}

The output shows the dependencies installed in the environment to
reproduce the bug and the verdict of the test oracle from pytest. In
this case, the execution of the bug-revealing test throws a runtime
error. Note that this test passes as it reproduces the intended bug.

%% Afterwards, the issue
%% id is printed and the exception that has been caught is printed to
%% demonstrate the bug. At the end, the test is shown to be passed,
%% signifying the successful reproduction of the bug.

%% \Fix{Please review and revise}

\subsubsection{Running FreeFuzz~\cite{wei2022free} on \tname.}

Tool execution proceeds in two steps: (1) Creation of Docker container
associated with a tool and (2) Execution of the tool.  Let us
demonstrate this process on FreeFuzz~\cite{wei2022free}, an API-level
fuzzer for DL libraries. The following script executes the first step
of the two-step integration method.

%i.e., it builds a Docker container for FreeFuzz:

% \Fix{walkthrough an example. Show input and part of output. Explain
%   the output.}
%% The user needs to execute two steps to integrate a testing tool (e.g.,
%% FreeFuzz) on \tname.
%% 1. Create a docker container for the tool
%% 2. A script that takes the name of the library as an argument and run FreeFuzz on the library


\begin{lstlisting}[language=bash]
$> cd tool-integration/FreeFuzz && bash install_freefuzz_docker.sh
\end{lstlisting}

\noindent
The script \CodeIn{install\_freefuzz\_docker.sh} contains the
following instructions:

\begin{lstlisting}[language=bash]
#!/bin/bash
git clone https://github.com/ise-uiuc/FreeFuzz.git
docker build -t freefuzz .
docker run --name freefuzz --gpus all -d freefuzz:latest
docker exec -it freefuzz mongorestore dump/
\end{lstlisting}

%\Fix{I am here -Marcelo}
%% Once the docker container is built and is running, providing the name
%% of the container and the location of the script that can execute the
%% tool (either absolute path or relative to the root of the repository)
%% to the command

The second step runs the \tname's command \CodeIn{run-tool}, which
proceeds in two steps (1) reproduce the bug separately, as done with
command \CodeIn{run-test}, and (2) then update the environment inside
the docker container of the tool to run the tool with the script
provided.\Fix{is the container destroyed after that? you are changing
  the environment for a certain bug, right?! - Abid: the dependencies are installed inside the docker, but after running the dependencies are not reverted back. if another bug is to be reproduced, installing the new dependencies will simply overwrite the previous dependencies} The first step is a
sanity check to assure that the bug can indeed be reproduced. For
example, the following command runs FreeFuzz on the enviroment of the
bug associated with the issue 117033 from \torch.

\begin{lstlisting}[language=bash]
  $> run-tool --container freefuzz --library-name pytorch --bug-id 117033 --run-script tool-integration/FreeFuzz/run_freefuzz_docker.sh
\end{lstlisting}

The output looks like the following:

\begin{lstlisting}[language=bash]
  ...
  [Reproducing the bug]
  ====== test session starts ======
  platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
  ...
  IndexError: list index out of range
  ====== 1 passed in 1.09s ======
  ...
  [Setting up FreeFuzz]
  Updating environment in the container of the testing tool
  Successfully copied 16.4kB to freefuzz:/tmp/issue_117033
  __pycache__  install_environment.sh  reproduce_bug.sh  requirements.txt  test_issue_117033.py
  ...
  [Running FreeFuzz]
  Running the testing tool on the environment of the bug
  Testing on  ['torch']
  torch.log2
  ...
  torch.nn.MaxUnpool2d
  ../aten/src/ATen/native/cuda/MaxUnpooling.cu:47: max_unpooling2d_forward_kernel: block: [0,0,0], thread: [1,0,0] Assertion `maxind >= 0 && maxind < outputImageSize` failed.
  ../aten/src/ATen/native/cuda/MaxUnpooling.cu:47: max_unpooling2d_forward_kernel: block: [0,0,0], thread: [2,0,0] Assertion `maxind >= 0 && maxind < outputImageSize` failed.
  ...
\end{lstlisting}

% \Fix{Consider the comments from Saikat, please.}
Initially the script reproduces the bug, then it updates the environment inside the docker container of FreeFuzz to match the enviroment of the bug. Then it proceeds to run the entire testing tool on the enviroment. After the execution completes, the output can be checked for all failures produced by FreeFuzz (saved inside the container) and manually inspected to see if any failure that can reveal the bug in question was generated by the tool. In this case, the failure generated by FreeFuzz could not generate a failure that can reveal this bug since the API "torch.\_dynamo.export" is not supported by FreeFuzz.

\Fix{Please review and revise}

\section{\tname: Use cases}

\Fix{@Saikat, can you please write 1-2 sentences about each use case here?}

\subsection{Evaluating DL library testing techniques}
\subsection{Fault Detection}
\subsection{Program Repair}


\section{Related Work} 
Several bug datasets have been proposed in literature for general software. Some
popular examples includes software-artifact infrastructure repository
(SIR)~\cite{do2005supporting} which was one of the bug datasets containing 81
artificial faults in projects across multiple languages,
Defects4J~\cite{just2014defects4j} which includes 357 real bugs across five
large real-world Java projects, BugsInPy~\cite{widyasari2020bugsinpy} containing
493 real bugs from 17 real-world Python programs, and
BugsJS~\cite{vancsics2020relationship} containing 453 bugs across 10 Javascript
projects.
%
Google's FuzzBench~\cite{metzman2021fuzzbench} provides an evaluation platform
for comparing general purpose fuzzers. Magma~\cite{hazimeh2020magma} is another
fuzzing benchmark built by \emph{front-porting} real bugs in latest version of
projects. In contrast to these general purpose benchmarks, \tname includes
reproducible bugs for Deep Learning libraries like PyTorch and TensorFlow, and
allows systematic benchmarking for DLL fuzzers. 

% Defects4J\cite{just2014defects4j}
% BugsInPy~\cite{widyasari2020bugsinpy} Tests4Py~\cite{smytzek2024tests4py}
% Denchmark~\cite{kim2021denchmark}

\section{Conclusion and Future Work}

\Fix{Marcelo will write a paragraph}

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

%%
%% If your work has an appendix, this is the place to put it.
\appendix

\section{Walkthrough}

\Fix{}



\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
