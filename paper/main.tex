\documentclass[sigconf,screen]{acmart}

\setcopyright{none} % to remove the copyright notice
\settopmatter{printacmref=false} % to remove the ACM Reference Format
\renewcommand\footnotetextcopyrightpermission[1]{} % to remove copyright box


\usepackage{multirow}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{listings}
\usepackage{enumitem}
\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  frame=single,
  breaklines=true,
  postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
  escapeinside={(*@}{@*)},
}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
%% \setcopyright{acmlicensed}
%% \copyrightyear{2018}
%% \acmYear{2018}
%% \acmDOI{XXXXXXX.XXXXXXX}
%% %% These commands are for a PROCEEDINGS abstract or paper.
%% \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%%   conference title from your rights confirmation email}{June 03--05,
%%   2018}{Woodstock, NY}
%% %%
%% %%  Uncomment \acmBooktitle if the title of the proceedings is different
%% %%  from ``Proceedings of ...''!
%% %%
%% %%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%% %%  June 03--05, 2018, Woodstock, NY}
%% \acmISBN{978-1-4503-XXXX-X/2018/06}
%% \acmBooktitle{Companion Proceedings of the 33rd ACM Symposium on the Foundations
%% of Software Engineering (FSE '25), June 23--27, 2025, Trondheim, Norway}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

\input{macros}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%% The "title" command has an optional parameter, allowing the author
%% to define a "short title" to be used in page headers.

\newcommand{\myshort}{\tname~: A Database of Reproducible Bugs in
  Deep Learning Libraries}

\title[\myshort{}]{\myshort{} to Enable Systematic Evaluation of
  Testing Techniques}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
%% \author{Ben Trovato}
%% \authornote{Both authors contributed equally to this research.}
%% \email{trovato@corporation.com}
%% \orcid{1234-5678-9012}
%% \author{G.K.M. Tobin}
%% \authornotemark[1]
%% \email{webmaster@marysville-ohio.com}
%% \affiliation{%
%%   \institution{Institute for Clarity in Documentation}
%%   \city{Dublin}
%%   \state{Ohio}
%%   \country{USA}
%% }

%% \author{Lars Th{\o}rv{\"a}ld}
%% \affiliation{%
%%   \institution{The Th{\o}rv{\"a}ld Group}
%%   \city{Hekla}
%%   \country{Iceland}}
%% \email{larst@affiliation.org}

\author{M. M. Abid Naziri}
\affiliation{%
  \institution{NC State University, USA}
  %\city{Rocquencourt}
  \country{}
}
\email{mnaziri@ncsu.edu}

\author{Aman Kumar Singh}
\affiliation{%
 \institution{Amrita Vishwa Vidyapeetham, India}
 %% \city{Doimukh}
 %% \state{Arunachal Pradesh}
 \country{}
}
\email{amanks@am.amrita.edu}

\author{Feiran Qin}
\affiliation{%
 \institution{NC State University, USA}
 %% \city{Doimukh}
 %% \state{Arunachal Pradesh}
 \country{}
}
\email{fqin2@ncsu.edu}

\author{Benjamin Wu}
\affiliation{%
 \institution{Purdue University, USA}
 %% \city{Doimukh}
 %% \state{Arunachal Pradesh}
 \country{}
}
\email{wu2059@purdue.edu}

\author{Saikat Dutta}
\affiliation{%
 \institution{Cornell University, USA}
 %% \city{Doimukh}
 %% \state{Arunachal Pradesh}
 \country{}
}
\email{saikatd@cornell.edu}

\author{Marcelo d'Amorim}
\affiliation{%
 \institution{NC State University, USA}
 %% \city{Doimukh}
 %% \state{Arunachal Pradesh}
 \country{}
}
\email{mdamori@ncsu.edu}


%% \author{Julius P. Kumquat}
%% \affiliation{%
%%   \institution{The Kumquat Consortium}
%%   \city{New York}
%%   \country{USA}}
%% \email{jpkumquat@consortium.net}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Naziri et al.}


%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  \sloppy
  AI-enabled applications are prolific today. Deep Learning~(DL)
  libraries, such as \torch{} and \tf{}, provide the building
  blocks for the AI components of these applications. As any piece of
  software, these libraries can be buggy.
  %% and those bugs can affect a
  %% great deal of applications using those libraries.
  An impressive number of bug-finding techniques to address this
  problem have been proposed, but the lack of a curated set of
  reproducible bugs in DL libraries hinders credible evaluation of
  these techniques. We present \tname, a database of curated
  reproducible bugs to fill that gap. Unique challenges exist in this
  context, such as installing drivers of specific CUDA versions to
  reproduce certain GPU-related bugs.
  %% enable credible  evaluation of bug-finding DL library testing techniques.
  Our dataset currently consists of \numbugs{} environments to
  reproduce bugs across three popular DL libraries, namely, \jax,
  \tf, and \torch.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
%% \begin{CCSXML}
%% <ccs2012>
%%  <concept>
%%   <concept_id>00000000.0000000.0000000</concept_id>
%%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%%   <concept_significance>500</concept_significance>
%%  </concept>
%%  <concept>
%%   <concept_id>00000000.00000000.00000000</concept_id>
%%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%%   <concept_significance>300</concept_significance>
%%  </concept>
%%  <concept>
%%   <concept_id>00000000.00000000.00000000</concept_id>
%%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%%   <concept_significance>100</concept_significance>
%%  </concept>
%%  <concept>
%%   <concept_id>00000000.00000000.00000000</concept_id>
%%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%%   <concept_significance>100</concept_significance>
%%  </concept>
%% </ccs2012>
%% \end{CCSXML}

%% \ccsdesc[500]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
%% \ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
%% \ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
%% \ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Deep learning libraries, testing, benchmarking}
%% %% A "teaser" image appears between the author and affiliation
%% %% information and the body of the document, and typically spans the
%% %% page.
%% \begin{teaserfigure}
%%   \includegraphics[width=\textwidth]{sampleteaser}
%%   \caption{Seattle Mariners at Spring Training, 2010.}
%%   \Description{Enjoying the baseball game from the third-base
%%   seats. Ichiro Suzuki preparing to bat.}
%%   \label{fig:teaser}
%% \end{teaserfigure}

%% \received{20 February 2007}
%% \received[revised]{12 March 2009}
%% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle
%% add page numbers
\pagestyle{plain}


\section{Introduction}

\sloppy Several application domains (e.g., transportation and
medicine) use AI as part of their solutions. Deep Learning~(DL)
libraries, such as \jax, \torch{}, and \tf{}, provide the building
blocks for the AI components of these applications.  Unfortunately, as
any piece of software, these libraries contain bugs. An impressive
number of techniques have been recently proposed to find bugs in these
libraries~\cite{wei2022free,xie2022docter,deng2022fuzzing,pham2019cradle,guo2020audee,wang2020deep,gu2022muffin,deng2023large,liu2023nnsmith,liu2023neuri,shi2023acetest,deng2023largeedge},
however we observe these techniques propose an independent evaluation
methodology. These techniques do \emph{not} use a reference database
of reproducible bugs to evaluate their effectiveness. The lack of an
evaluation standards is a serious obstacle to a fair and rigorous
comparison of techniques and hinders research progress.

%%~\cite{just2014defects4j,smytzek2024tests4py,widyasari2020bugsinpy,kim2021denchmark}
Although many datasets of reproducible bugs have been proposed in the
literature, there is a lack of solutions satisfying the following
criteria:

\begin{enumerate}[leftmargin=0.2in]
\item developers should be able to write tests scripts in Python;
\item ~they should be able to handle nighly builds and specific
  versions of CUDA (\textsection~\ref{sec:challenges});
\item they should be able to integrate fuzzing tools in the framework.
\end{enumerate}  

Note that DL libraries are written in Python. Existing datasets of
reproducible bugs exist in Python (e.g.,
BugInPy~\cite{widyasari2020bugsinpy} and
Tests4Py~\cite{smytzek2024tests4py}), but they fail to satisfy the
second or the third requirement. For example, BugsInPy and Tests4Py do
not support artifacts in Docker, which are necessary to modify CUDA
drivers in the guest OS to reproduce specific GPU-related
bugs.\footnote{BugsInPy and Tests4Py use python virtual environments,
e.g., venv and pyenv.} More importantly, a dataset of Deep Learning
Libraries needs to support the integration of new fuzzing tools. The
central purpose of such dataset is to \emph{enable the systematic
evaluation of testing techniques}.

%% \Fix{Clarify how the
%%   "evaluation set" used by the existing works is different from the
%%   "reference set" that we are proposing. Also, need more compelling
%%   reason/some sort of evaluation of other fuzzers to complement the
%%   motivation}

% \Fix{Add a "Tool availablity" section and move the details there, upload the video to YouTube, archive 
% the tool at zenodo}

We present \tname, a database of curated reproducible bugs to enable
credible evaluation of DL library testing techniques. \tname\ is
equipped with a command-line interface to enable researchers to
analyze and reproduce bug instances.  \tname\ has been under active
development since March 21, 2024, the day of the first commit in its
GitHub repository. Two UROP students, two PhD students, and two
faculty were involved in the work during this
period. Section~\ref{sec:tool-availability} details tool availability.

%\Fix{Need to add text regarding differentiating the tool from BugsInPy}

\section{Tool Availability}
\label{sec:tool-availability}
\tname\ is publicly available from the following URL:

\url{https://github.com/ncsu-swat/bugsindlls}

The following video demonstrates \tname:

\url{https://www.youtube.com/watch?v=NslNWrULT1c}

The archived version at Zenodo is available with this DOI:

\url{https://doi.org/10.5281/zenodo.15064163}\\

% \Fix{Upload to YouTube with voiceover and add link here, archive 
% the tool at zenodo}

\section{Objects and Methods}

\begin{table}
  \centering
  \caption{\label{table:bug-characterization}Characterization of
    reproducible bugs from \tname.}
  %%   \Fix{Is this up to date? I have seen
  %% lots of activity recently but these numbers remain the
  %% same - Fixed (Abid)}\Fix{Can we add info about whether Python and/or C code causes
  %%     the bug? - Abid: we report how many c,python and cuda native files are buggy in total but not per bug, so currently no}\Fix{Can we add info on whether it is GPU related
  %%     (i.e., only manifests in GPUs)? - Abid: venv-docker is the same
  %%     as cpu-gpu}}
\begin{tabular}{l|rrr}
  \toprule & \multirow{2}{*}{\#} & \multicolumn{1}{r}{build} &
  \multicolumn{1}{r}{enviroment} \\ & & release-nightly & venv-docker
  \\ \cmidrule(lr){2-4} \jax{} & 46 & 45-1 & 45-1 \\ \torch{} & 37 &
  18-19 & 21-16 \\ \tf{} & 24 & 23-1 & 24-0 \\ \midrule
  \multicolumn{1}{c|}{$\Sigma$} & \numbugs{} & 86-21& 90-17
  \\ \bottomrule
\end{tabular}
\end{table}

This section describes the criteria for selecting libraries and bugs
(Section~\ref{sec:selection-criterion}), the challenges for bug
reproduction (Section~\ref{sec:challenges}), and the method we
followed to create bug instances (Section~\ref{sec:method}).

\tname\ contains \numbugs{} instances of reproducible bugs
representing three popular DL libraries, namely, \jax, \tf, and
\torch. Table~\ref{table:bug-characterization} shows the list of bug
instances for each supported library. Column ``\#'' shows the number
of bug instances, column ``build'' shows the breakdown of instances
for each kind of build (release or nightly), and column
``environment'' shows the breakdown of instances for each kind of
environment to reproduce the bug (venv or docker). Co-incidentally,
all bugs that require a GPU build are reproduced using a docker
container, i.e. the column venv-docker matches would have been
observed with a column CPU-GPU, to indicate if the bug is can be
reproduced with a CPU or if it requires a
GPU. Table~\ref{table:bug-stats} shows the breakdown error types for
the bugs in our dataset. The row ``Incorrect Output'' is listed first
as it requires a distinct kind of oracle comparing consistency of the
outputs of test runs on a CPU and on GPU. In total, we have
\numbugtypes\ different types of bug manifestations across the three
libraries.

\subsection{Selection Criteria}
\label{sec:selection-criterion}

% \Fix{Abid, explain the rationale for selecting jax, torch, and tf.}

\textbf{Libraries.}~\torch~and \tf are popular DL libraries
intensively used in the literature. \jax\ is a newer library that has
been rapidly gaining popularity. All these libraries are open source
with active communities supporting their maintenance. From these three
libraries, we selected issue reports from specific periods in their
respective issue trackers. For \torch, we selected issues from the
period between \torchfrom and \torchto. For \tf, we selected issues
from the period between \tffrom and \tfto. For \jax, we selected
issues from the period between \jaxfrom and \jaxto. We have chosen
different periods for each library depending on the number of issues
reported in the issue tracker in that period. For example, \tf has a
much longer period compared to the others because the frequency of
issues reported in the issue tracker is lower.

%% We chose these three popular datasets in order to enhance the process
%% of developing testing tools around these libraries.

% \Fix{Abid, explain the criterion for selecting the bugs (e.g., it
  % contains test code showing the problem).}

% \Fix{Need to add what labels were used, which period we took the issues from,
% how many issues we looked at, define "bug reports", mention how many bugs had 
% been filtered with each criterion}

\textbf{Bugs.}~We focus on issue reports that have been accepted by
the developer community as true bugs. We determine this by checking
whether the issue reports have the label "bug" and have been fixed with pull
requests linked to them. We then create a filter that incorporates
the periods mentioned above, the label, the issue status being closed
and the presence of a linked pull request. We found \numtorchfiltered{}
issues for \torch, \numtffiltered{} issues for \tf, and \numjaxfiltered{}
issues for \jax after filtering the issues. \torch has the highest number
of issues due to a lack of a label for bugs in the issue tracker.
%%
Then, we manually inspect the filtered issues to ensure that they
contain enough information to reproduce the bug, i.e., code to
reproduce the bug, dependencies required for the bug reproduction,
clear description of the desired behavior, etc. We discard bugs that
are not in the core functionality of the library e.g. documentation
issues, feature requests, etc. After this stage of manual inspection,
we end up with \numtorchinspected{} issues for \torch,
\numtfinspected{} issues for \tf, and \numjaxinspected{} issues for
\jax.
%
Finally, we assess the reproducibility of the bug by creating a
virtual environment that contains the necessary dependencies to
reproduce the bug.  Once we can successfully reproduce the bug by
creating the enviroment detailed in the report, we include the
corresponding artifacts in our dataset. Throughout this process, we
obtain \textbf{\numjaxbugs{}}, \textbf{\numtorchbugs{}}, and
\textbf{\numtfbugs{}} bug instances for \jax, \torch, and \tf,
respectively.

% Among these confirmed bug reports, we choose
% the ones that have enough information to reproduce the bugs, i.e.,
% code to reproduce the bug, dependencies required for the bug
% reproduction
% \Fix{Abid, can you confirm that environment here is not
%   the same as venv-docker that we are using on Table 1? instead it
%   means the version of CUDA, libraries, etc.? If so, avoid (re)using the
%   word environment.}
% , clear description of the desired behavior,
% etc. 

%% \Fix{Abid: Added what labels were used, which period we took the issues from,
%% how many issues we looked at, defined "bug reports", mentioned how many bugs had 
%% been filtered with each criterion. Please review and revise}

\begin{table}
  \centering
  \caption{\label{table:bug-stats}Different bug manifestation types
    from \tname.}
  \begin{tabular}{lrrrrr}\toprule
    Bug manifestation type &Jax &Pytorch &Tensorflow &Total \\\midrule
    Incorrect Output &14 &11 &5 &30 \\
    Internal Exception &11 &9 &4 &24 \\
    Value Error &7 &0 &4 &11 \\
    Type Error &0 &1 &8 &9 \\
    Attribute Error &3 &2 &3 &8 \\
    Index Error &0 &7 &1 &8 \\
    Runtime Error &2 &0 &2 &4 \\
    Memory Error &2 &1 &0 &3 \\
    Invalid Argument Error &0 &2 &1 &3 \\
    Floating Point Error &1 &1 &0 &2 \\
    Error Not Raised &1 &0 &1 &2 \\
    Parse Error &0 &2 &0 &2 \\
    Runtime Warning &1 &0 &0 &1 \\
    Not Implemented Error &1 &0 &0 &1 \\
    Assertion Error &1 &0 &0 &1 \\
    Segmentation Fault &1 &0 &0 &1 \\
    Floating Point Exception &1 &0 &0 &1 \\
    Aborted &0 &1 &0 &1 \\
    \midrule
    $\Sigma$ &\numjaxbugs{} &\numtorchbugs{} &\numtfbugs{} &\numbugs{} \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Challenges}
\label{sec:challenges}

\textbf{Handling bugs from nightly builds requires saving wheels.}
Many of the reproducible bugs rely on the libraries' nightly builds,
which are available for a limited amount of time. To ensure that bugs
reported on these builds remain reproducible, it is necessary to save
Python wheel files (.whl)~\cite{wheels} for the correspondind builds.

\vspace{0.5ex}\noindent\textbf{Handling bugs in specific GPU-CUDA
  versions requires OS changes:} Some GPU-related bug instances can
only be reproduced with specific versions of CUDA~\cite{cuda},
NVIDIA's platform and API for programming GPUs. These instances
require specific CUDA drivers to be installed on the system. Python
virtual enviroments are not designed to enable change of OS drivers.
% GPU builds\Fix{I don't understance GPU builds. do you mean
%   GPU vendor/version? give example} and specific CUDA builds. 
% Changing different virtual environments does not change the CUDA
% versions\Fix{Do you mean ``it is not possible to change the CUDA (or
%   GPU) version with Python's virtual environments''?} of the
% system. 
% That is why we use Docker when a bug relies on a specific
% version of CUDA to be installed in the system.
We use Docker containers~\cite{merkel2014docker} for the bug instances
that require changes in drivers.

\subsection{Method}
\label{sec:method}

The following steps show the method we use to create bug instances:

\begin{enumerate}[leftmargin=0.5cm]
  \item Select an issue following the criteria defined in Section~\ref{sec:selection-criterion};
  \item Identify the version of the library used to report the bug along with other dependencies;
  \item Write a \CodeIn{requirements.txt} file with the list of dependencies to reproduce the bug;
  \item Create a file showing the code that causes the bug (for documentation);
  \item Create a file with a \CodeIn{pytest} test case that passes if
    the code triggers the buggy behavior documented in the issue;
    %% A python file is created that contains the bug revealing code, along with a test (pytest) that passes upon successfully trigerring the buggy behavior.
  \item If the bug depends on a CUDA-specific build, write a
    Dockerfile to create a container including the needed CUDA-driver
    and the other artifacts mentioned above (e.g., test file and
    requirements);
    %% along with the requirements.txt file and the python
    %% script copied into it.
  \item Write a script that creates the virtual environment (either
    Python's \CodeIn{Conda Enviroment}~\cite{venv} or Docker container) and runs
    the test on it;
  \item Create a directory \CodeIn{<lib>/<bug-id>} containing all the
    artifacts mentioned above, where \CodeIn{lib} refers to the
    affected library and \CodeIn{bug-id} is a fresh identifier of the
    bug instance.
    %% All files are put into a single self-contained folder with the issue id in the name so that the tools exposed by the framework of our tool can find and execute the the bug reproduction script.
\end{enumerate}

\textbf{Test Oracles.}~
The bug reproduction code includes python tests (pytest). The test
oracle is satisfied when the bug is reproduced successfully. For bugs
that throw an unexpected exception, the test catches the exception and
reports the exception details along with a pass status. For bugs that
result in incorrect output value, assertions are placed in the code to
expect the incorrect output value, hence passing the test when the bug
occurs. For bugs that crash after raising a signal, the code snippet
is placed in a seperate file and executed, while the file contaning
the actual test asserts the presence of the signal raised by the
code. Upon successful reproduction, the original code crashes raising
the proper signal and the assertion passes.
% \Fix{Please review and revise}

\textbf{Example}
% ~\Fix{Can you show here the artifacts associated to bug
%   120903 that you will use in the next section? If it takes too much
%   space, show only directory with files and the test case. - Abid: by the test case, do you mean the python code used to reproduce the bug?} 
  The issue no. 120903 in \torch\ has the following test code that checks the oracle:

\begin{lstlisting}[language=python,basicstyle=\small,]
def test_f():
  input_data = torch.randn(3, 4, 5, 6)
  scale = np.array([0.1, 0.2, 0.3, 0.4])
  zero_point = torch.tensor([1, 2, 3, 4], dtype=torch.int32)
  axis = 1
  quant_min = 0
  quant_max = 255
  with pytest.raises(RuntimeError) as e_info:
      output = torch.fake_quantize_per_channel_affine(input_data, torch.from_numpy(scale), zero_point, axis, quant_min, quant_max)
  print(f'{e_info.type.__name__}: {e_info.value}')
\end{lstlisting}

In this example, the unexpected behavior is a \CodeIn{RuntimeError} thrown by the call to the \CodeIn{fake\_quantize\_per\_channel\_affine} API from \torch. Since the test oracle should pass when the bug is successfully reproduced, the code catches the exception and prints information about it, and upon catching the exception successfully, the test passes. If the bug reproduction fails i.e. the exception is not thrown or a different exception is thrown, the test will fail.

\section{\tname} % : Database of Bugs in DL libraries

This section presents the interface of \tname\ and walks the reader
through a demonstration showing the tool's functionality.


\subsection{Interface}

\begin{table}
  \centering
  \caption{\label{table:bug-interface}\tname's bug interface.}
\begin{tabular}{lp{6.5cm}}
  \toprule
  \textbf{command} & \textbf{description} \\
  \midrule
list-tests & List the tests available on this dataset\\
run-test & Runs one test\\
run-tests & Runs all the tests\\
run-tool & Runs a testing tool in a given buggy environment \\
show-info & Shows information about the tests available on this benchmark\\
stats & Shows statistics about this dataset (e.g., number of tests
that require GPU, number of tests that reproduce bugs in C or Python
code, etc.)\\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{table:bug-interface} shows the list of commands in the
\tname's interface along with a short description for these
commands. In the following we demonstrate \tname.
% \Fix{explain that the test passes if the bug can be reproduced}

\subsection{Usage}
\label{sec:usage}

To enable system-wide access of the framework, it is necessary to add
the directory \CodeIn{/framework} to the \CodeIn{PATH} environment
variable. Run the following commands for that:

\begin{lstlisting}[language=bash,basicstyle=\small,]
$> git clone git@github.com:ncsu-swat/bugsindlls.git
$> cd bugsindlls
$> export PATH=$PATH:`pwd`/framework
\end{lstlisting}


\subsubsection{Running one bug instance}

% \Fix{walkthrough an example. Show input and part of output. Explain
%   the output.}


Let us use the bug 120903~\cite{torch120903} from \torch\ to demonstrate the
command \CodeIn{run-test}, which runs a test to reproduce a bug on an
environment with the necessary dependencies installed. Use the
following command to reproduce the bug:

\begin{lstlisting}[language=bash,basicstyle=\small,]
$> run-test --library-name pytorch --bug-id 120903
\end{lstlisting}

\noindent
Execution of this command produces the following output:

\begin{lstlisting}[language=bash,basicstyle=\small,]
... 
Versions of relevant libraries:
[pip3] numpy==1.26.4
[pip3] torch==2.2.0+cpu
[conda] torch                     2.2.0+cpu                pypi_0    pypi
====== test session starts ======
platform linux -- Python 3.10.0, pytest-8.2.0, pluggy-1.5.0
...
test_issue_120903.py Pytorch issue no. 120903
Seed:  120903
RuntimeError: !needs_dynamic_casting<func_t>::check(iter) INTERNAL ASSERT FAILED at "../aten/src/ATen/native/cpu/Loops.h":310, please report a bug to PyTorch. 
====== 1 passed in 0.96s ======
\end{lstlisting}

The output shows the dependencies installed in the environment to
reproduce the bug and the verdict of the test oracle from pytest. In
this case, the execution of the bug-revealing test throws a runtime
error. Note that this test passes as it reproduces the intended bug.

%% Afterwards, the issue
%% id is printed and the exception that has been caught is printed to
%% demonstrate the bug. At the end, the test is shown to be passed,
%% signifying the successful reproduction of the bug.

%% \Fix{Please review and revise}

% \Fix{What evaluation can be added to this section?}
\subsubsection{Running FreeFuzz~\cite{wei2022free} on \tname.}\label{sec:running-free-fuzz}

A developer needs to provide three scripts to integrate a fuzzing
tool: (1) a script that contains commands to run the tool (e.g. \CodeIn{run\_freefuzz\_docker.sh}); (2) a
preprocessing script to extract error types and buggy APIs for a
specific library version (\CodeIn{preprocess.py}), and (3) a postprocessing script to match the
execution log with the expected errors (\CodeIn{postprocess.py}) needs to be provided.
% \Fix{name
%   these scripts here and refer back to them in the text below} 
The
directory \CodeIn{tool-integration} that contains templates for these
scripts and their instantiations for FreeFuzz~\cite{wei2022free}, a
popular API fuzzer for DL libraries.
% \Fix{remove this paragraph. Explain the steps when they appear. It is difficult
%   to follow}
% \Fix{up to here}

We demonstrate the integration of FreeFuzz~\cite{wei2022free}. The
following script creates a Docker container associated with FreeFuzz.

\begin{lstlisting}[language=bash,basicstyle=\small,keywords={}]
$> cd tool-integration/FreeFuzz && bash install_freefuzz_docker.sh
\end{lstlisting}

\noindent
The script \CodeIn{install\_freefuzz\_docker.sh} build the docker
container that encapsulates FreeFuzz.

%% 
%% The script contains the following instructions:
%% % \Fix{add brief explanation for what it does}:
%% \begin{lstlisting}[language=bash,basicstyle=\small,keywords={}]
%% #!/bin/bash
%% git clone https://github.com/ise-uiuc/FreeFuzz.git
%% docker build -t freefuzz .
%% docker run --name freefuzz --gpus all -d freefuzz:latest
%% docker exec -it freefuzz mongorestore dump/
%% \end{lstlisting}

The following script runs FreeFuzz all bugs associated with version
2.2.0 of \torch.  The command \CodeIn{run-tool} takes the library
version as an input, along with the name of the docker container
(already created), the name of the library, and the user-provided
script containing the commands to run the tool
(\CodeIn{run\_freefuzz\_docker.sh}. The script reports on output how
many bugs are reproducible with this library version were successfully
reproduced by the given tool.  Tool execution proceeds as
follows. First, a Docker container associated with the tool is created using
\CodeIn{install\_freefuzz\_docker.sh}. Next, a reproducible bug is tested using
the specified library version as input to verify the setup.
Following this, the preprocessing script (\CodeIn{preprocess.py}) is executed 
to extract error types and identify buggy APIs for the given library version.
The environment inside the Docker container is then updated to match the specified version, 
ensuring compatibility before running the fuzzing tool. Once execution is complete, a postprocessing 
script (\CodeIn{postprocess.py}) is run to analyze the execution log against the ground truth, 
determining whether the bugs were successfully reproduced.


\begin{lstlisting}[language=bash,basicstyle=\small,keywords={}]
$> run-tool --container freefuzz --library-name pytorch \ 
   --use-library-version 2.2.0 \
   --run-script tool-integration/FreeFuzz/run_freefuzz_docker.sh
\end{lstlisting}

% The script \CodeIn{run\_freefuzz\_docker.sh} contains instructions to run the preprocessing script, the tool, and the postprocessing script.
The output looks like the following:

\begin{lstlisting}[language=bash,basicstyle=\small,keywords={}]
Using bug-id 120903 for library pytorch version 2.2.0
...
====== test session starts ======
platform linux -- Python 3.10.0, pytest-8.2.0, pluggy-1.5.0
...
RuntimeError: !needs_dynamic_casting<func_t>::check(iter) INTERNAL ASSERT FAILED at "../aten/src/ATen/native/cpu/Loops.h":310, please report a bug to PyTorch.
====== 1 passed in 1.09s ======
...
Updating environment in the container of the testing tool
Successfully copied 16.4kB to freefuzz:/tmp/issue_120903
...
Running the testing tool on the environment of the bug
APIs under test:
torch.fake_quantize_per_channel_affine
torch.all
torch.compile
Testing on  ['torch']
torch.atanh
...
No violation of precision-oracle in the compare-bug category
No violation of precision-oracle in the potential-bug category
No violation of cuda-oracle in the compare-bug category
No violation of cuda-oracle in the potential-bug category
No violation of crash-oracle in the compare-bug category
No violation of crash-oracle in the potential-bug category
-> torch.fake_quantize_per_channel_affine did not face any failures
-> torch.all did not face any failures
-> torch.compile did not face any failures
Reproduced 0 out of 3 bugs
\end{lstlisting}

% \Fix{Consider the comments from Saikat, please.}
% Initially, the script reproduces the bug and updates the environment
% inside the docker container of FreeFuzz to match the enviroment of the
% bug. Then, it proceeds to run the entire testing tool on the
% enviroment.

% \Fix{Abid, this does not make sense to me. If you are checking this
%   manually, you could run FreeFuzz separately and check manually. What
%   is the point?! You need to provide a list of bugs and check
%   automatically how many the tool can find (based on some criteria,
%   e.g, match the error message in Assertion or some n frames in the
%   stack (for crash)). --Marcelo}
% After the execution completes, the output can be
% checked for all failures produced by FreeFuzz (saved inside the
% container) and manually inspected to see if any failure that can
% reveal the bug in question was generated by the tool. In this case,
% the failure generated by FreeFuzz could not generate a failure that
% can reveal this bug since the API "torch.\_dynamo.export" is not
% supported by FreeFuzz.
% \Fix{This is bad in two way: (1) we did not give
%   a chance to Freefuzz; unsupported API (2) it does not show any case
%   where the tool can reproduce the bug (and demo that our matchers
%   work). I hope you can do this on video or show another example to
%   complement this or modify this.}

\Fix{Abid: Added text to motivate adding support for these APIs, please revise}

The execution log shows that the tool did not reproduce any of the
three bugs that were expected to be reproduced since the three buggy APIs \CodeIn{torch.fake\_quantize\_per\_channel\_affine}, \CodeIn{torch.all}, and \CodeIn{torch.compile} are not supported by FreeFuzz. The outputs of the tool are saved in the container and can be inspected manually to further inspect the execution log. From this result, the developer can decide to add support for these three APIs in FreeFuzz and run the tool again to check if the bugs can be reproduced now. This will allow developers to decide on which APIs to add support for.

\subsection{Contributions}

\tname\ allows users to contribute to the dataset by adding new bug
instances. This opens up the tool to the community to help in
expanding the dataset and improving the quality of the dataset.
The following steps show how to contribute a new bug
instance:

\begin{enumerate}[leftmargin=0.5cm]
  \item Identify issues in the issue tracker of the library of interest;
  \item Create an issue in the repository of \tname\ with the template
    "Reproduce Bug" (available in the repository);
  \item Create a branch linked to the issue;
  \item Add a self-contained bug reproduction script in a sub-directory
    named with the GitHub issue identifier under the directory of the library;
  \item Prepare the execution environment (Docker containers~\cite{merkel2014docker} for CUDA-dependent
    bugs, \CodeIn{Conda Enviroment}~\cite{venv} for others);
  \item Follow the steps in \ref{sec:method} to create the bug reproduction script;
  \item Update the spreadsheets of the library with bug details;
  \item Create a pull request to the main branch.
\end{enumerate}

%% \Fix{Abid: Added, please revise}
%% \section{Use case: Evaluation of testing techniques}
%% The primary use case of \tname\ is the evaluation of testing
%% techniques. Dozens of API- and model-level testing techniques have
%% been proposed in the literature recently and it is imperative to
%% evaluate them using a standard
%% benchmark. Section~\ref{sec:running-free-fuzz} elaborates on how to
%% integrate a testing tool, such as FreeFuzz~\cite{wei2022free}, into
%% \tname. 


%% We anticipate two major use cases for \tname: (1) \textbf{} and (2) \textbf{evaluation of repair
%%   techniques}. Considering repair, as testing techniques mature it becomes
%% important to identify candidate repairs that can be recommended to
%% developers when reporting issues in bug trackers. With \tname\ it is
%% possible to identify if the problem has been resolved. 
%% \Fix{The concept of repairing has not been tied to any commands in the tool. Suggestions include
%% a way to recreate an enviroment with the bug fixed, or a way to check if the bug has been fixed in a new version of the library.}

\section{Related Work} 

Several bug datasets have been proposed in literature for general
software. Some popular examples includes software-artifact
infrastructure repository (SIR)~\cite{do2005supporting} which was one
of the bug datasets containing 81 artificial faults in projects across
multiple languages, Defects4J~\cite{just2014defects4j} which includes
357 real bugs across five large real-world Java projects,
BugsInPy~\cite{widyasari2020bugsinpy} containing 493 real bugs from 17
real-world Python programs, and BugsJS~\cite{vancsics2020relationship}
containing 453 bugs across 10 Javascript projects.
%
Google's FuzzBench~\cite{metzman2021fuzzbench} provides an evaluation platform
for comparing general purpose fuzzers. Magma~\cite{hazimeh2020magma} is another
fuzzing benchmark built by \emph{front-porting} real bugs in latest version of
projects. In contrast to these general purpose benchmarks, \tname includes
reproducible bugs for Deep Learning libraries like PyTorch and TensorFlow, and
allows systematic benchmarking for DLL fuzzers. 

% Defects4J\cite{just2014defects4j}
% BugsInPy~\cite{widyasari2020bugsinpy} Tests4Py~\cite{smytzek2024tests4py}
% Denchmark~\cite{kim2021denchmark}

\section{Conclusion and Future Work}
Deep Learning (DL) Libraries (e.g., \torch) provide the basic blocks
for creating AI-enabled applications, which are becoming very
popular. Several testing techniques have been proposed in the
literature to find bugs in these libraries. Unfortunately, the lack of
a dataset of reproducible bugs in those libraries poses an important
barrier to the proper evaluation of these techniques. \tname{} fills
the gaps. It includes a total of \numbugs\ across three major DL
libraries. We plan to continue expanding this dataset and to explore
automated approaches to extract patches from issue trackers and
``frontport'' them~\cite{hazimeh2020magma} to a single version of the
library to facilitate testing.


\balance
\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

%%
%% If your work has an appendix, this is the place to put it.
\appendix

\section{Walkthrough}

Section~\ref{sec:usage} demonstrates usage of \tname. The reader can
also follow the steps in the README.md file on our GitHub repository.

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
