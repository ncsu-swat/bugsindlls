\documentclass[sigconf]{acmart}

\usepackage{multirow}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  frame=single,
  breaklines=true,
  postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation email}{June 03--05,
  2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/2018/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

\input{macros}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{\tname: A Database of Reproducible Bugs in Deep
  Learning Libraries to Enable Systematic Evaluation of Testing
  Techniques}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
%% \author{Ben Trovato}
%% \authornote{Both authors contributed equally to this research.}
%% \email{trovato@corporation.com}
%% \orcid{1234-5678-9012}
%% \author{G.K.M. Tobin}
%% \authornotemark[1]
%% \email{webmaster@marysville-ohio.com}
%% \affiliation{%
%%   \institution{Institute for Clarity in Documentation}
%%   \city{Dublin}
%%   \state{Ohio}
%%   \country{USA}
%% }

%% \author{Lars Th{\o}rv{\"a}ld}
%% \affiliation{%
%%   \institution{The Th{\o}rv{\"a}ld Group}
%%   \city{Hekla}
%%   \country{Iceland}}
%% \email{larst@affiliation.org}

\author{M. M. Abid Naziri}
\affiliation{%
  \institution{NC State University}
  %\city{Rocquencourt}
  \country{USA}
}
\email{mnaziri@ncsu.edu}

\author{Aman Kumar Singh}
\affiliation{%
 \institution{Amrita Vishwa Vidyapeetham}
 %% \city{Doimukh}
 %% \state{Arunachal Pradesh}
 \country{India}}
\email{amanks@am.amrita.edu}

\author{Feiran Qin}
\affiliation{%
 \institution{NC State University}
 %% \city{Doimukh}
 %% \state{Arunachal Pradesh}
 \country{USA}}
\email{fqin2@ncsu.edu}

\author{Benjamin Wu}
\affiliation{%
 \institution{Purdue University}
 %% \city{Doimukh}
 %% \state{Arunachal Pradesh}
 \country{USA}}
\email{wu2059@purdue.edu}

\author{Saikat Dutta}
\affiliation{%
 \institution{Cornell University}
 %% \city{Doimukh}
 %% \state{Arunachal Pradesh}
 \country{USA}}
\email{saikatd@cornell.edu}

\author{Marcelo d'Amorim}
\affiliation{%
 \institution{NC State University}
 %% \city{Doimukh}
 %% \state{Arunachal Pradesh}
 \country{USA}}
\email{mdamori@ncsu.edu}


%% \author{Julius P. Kumquat}
%% \affiliation{%
%%   \institution{The Kumquat Consortium}
%%   \city{New York}
%%   \country{USA}}
%% \email{jpkumquat@consortium.net}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Trovato et al.}


%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  \sloppy
  AI-enabled applications are prolific today. Deep Learning~(DL)
  libraries, such as \torch{} and \tf{}, provide the building
  blocks for the AI components of these applications. As any piece of
  software, these libraries can be buggy.
  %% and those bugs can affect a
  %% great deal of applications using those libraries.
  An impressive number of bug-finding techniques to address this
  problem have been proposed, but the lack of a curated set of
  reproducible bugs in DL libraries hinders credible evaluation of these
  techniques. We present \tname, a database of curated reproducible
  bugs to fill that gap.
  %% enable credible  evaluation of bug-finding DL library testing techniques.
  Our dataset currently consists of \numbugs{} environments to
  reproduce bugs across three popular DL libraries, namely, \jax,
  \tf, and \torch.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
%% \begin{CCSXML}
%% <ccs2012>
%%  <concept>
%%   <concept_id>00000000.0000000.0000000</concept_id>
%%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%%   <concept_significance>500</concept_significance>
%%  </concept>
%%  <concept>
%%   <concept_id>00000000.00000000.00000000</concept_id>
%%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%%   <concept_significance>300</concept_significance>
%%  </concept>
%%  <concept>
%%   <concept_id>00000000.00000000.00000000</concept_id>
%%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%%   <concept_significance>100</concept_significance>
%%  </concept>
%%  <concept>
%%   <concept_id>00000000.00000000.00000000</concept_id>
%%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%%   <concept_significance>100</concept_significance>
%%  </concept>
%% </ccs2012>
%% \end{CCSXML}

%% \ccsdesc[500]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
%% \ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
%% \ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
%% \ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Deep learning libraries, testing, benchmarking}
%% %% A "teaser" image appears between the author and affiliation
%% %% information and the body of the document, and typically spans the
%% %% page.
%% \begin{teaserfigure}
%%   \includegraphics[width=\textwidth]{sampleteaser}
%%   \caption{Seattle Mariners at Spring Training, 2010.}
%%   \Description{Enjoying the baseball game from the third-base
%%   seats. Ichiro Suzuki preparing to bat.}
%%   \label{fig:teaser}
%% \end{teaserfigure}

%% \received{20 February 2007}
%% \received[revised]{12 March 2009}
%% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

\sloppy Serveral important application domains (e.g., transportation,
medicine, etc.) use AI to optimize some element of their business
process. Deep Learning~(DL) libraries, such as \jax, \torch{}, and
\tf{}, provide the building blocks for the AI components of these
applications.  Unfortunately, as any piece of software, these
libraries contain bugs. An impressive number of techniques have been
recently proposed to find bugs in these libraries~\cite{wei2022free,xie2022docter,deng2022fuzzing,pham2019cradle,guo2020audee,wang2020deep,gu2022muffin,deng2023large,liu2023nnsmith,liu2023neuri,shi2023acetest,deng2023largeedge}, 
however we observe that the evaluation of
these techniques do \emph{not} use a reference set of reproducible
bugs to evaluate their effectiveness. The lack of evaluation standards
is an obstacle to the fair comparison of techniques, hindering
research progress.

This paper presents \tname, a database of curated reproducible bugs to
enable credible evaluation of DL library testing techniques. \tname\ is
equipped with a command-line interface to enable researchers to analyze
and reproduce bug instances.


\section{Objects and Methods}

\begin{table}
  \centering
  \caption{\label{table:bug-characterization}Characterization of reproducible bugs from \tname.}
\begin{tabular}{l|rrr}
  \toprule & \multirow{2}{*}{\#} & \multicolumn{1}{r}{build} &
  \multicolumn{1}{r}{enviroment} \\ & & release-nightly & venv-docker
  \\ \cmidrule(lr){2-4} \jax{} & 44 & 43-1 & 43-1 \\ \torch{} & 33 &
  15-18 & 19-14 \\ \tf{} & 25 & 24-1 & 25-0 \\ \midrule
  \multicolumn{1}{c|}{$\Sigma$} & \numbugs{} & 82-20 & 87-15
  \\ \bottomrule
\end{tabular}
\end{table}

This section describes the criteria for selecting libraries and bugs
(Section~\ref{sec:selection-criterion}), the challenges for bug
reproduction (Section~\ref{sec:challenges}), and the method we
followed to create bug instances (Section~\ref{sec:method}). The
\tname\ database consists of \numbugs{} instances of reproducible bugs
representing three popular DL libraries, namely, \jax, \tf, and
\torch. Table~\ref{table:bug-characterization} shows the list of bug
instances for each supported library. Column ``\#'' shows the number
of bug instances, column ``build'' shows the breakdown of bug
instances based on releases of the library against those based on
nightly builds, and column ``environment'' shows the breakdown of bug
instances that use Python's virtual environment against those that run
on a Docker container.

\subsection{Selection Criterion}
\label{sec:selection-criterion}

% \Fix{Abid, explain the rationale for selecting jax, torch, and tf.}

Amongst many DL libraries, the most popular libraries for developing AI-enabled applications and used in testing tools are \torch~and \tf. \jax~is a relatively newer entry to the list of libraries but it is rapidly gaining popularity as well. All three of these libraries are open source with active communities that support the libraries, making them very suitable for developing testing tools around. We chose these three popular datasets in order to enhance the process of developing testing tools around these libraries.

% \Fix{Abid, explain the criterion for selecting the bugs (e.g., it
  % contains test code showing the problem).}

For the bugs from these libraries, we focused on bug reports that have been accepted by the developer community as true bugs. We determine this by checking whether the bugs have appropriate labels and have been fixed with pull requests linked to them. Among these confirmed bug reports, we choose the ones that have enough information to reproduce the bugs i.e. code to reproduce the bug, enviroment required for reproduction, clear description of desired behavior etc. Once we can successfully reproduce the bug by creating the enviroment detailed in the report, we choose the bug to be included in our dataset.

\Fix{Please review and revise}

\subsection{Challenges}
\label{sec:challenges}

\textbf{Handling bugs from nighly builds requires saving wheels.}  Many
of the reproducible bugs rely on nightly builds of the
libraries. These builds are available for a limited amount of time. In
order to ensure bugs reported on these builds to remain reproducible,
it is imperative to save wheel files~\cite{wheels} for the
correspondind builds.

\textbf{Handling bugs in specific GPU-CUDA versions requires OS
  changes:} Many of the GPU-related bugs can only be reproduced with
library versions that are built with specific versions of CUDA.
These builds require those specific CUDA drivers to be installed on
the system, and the version of the CUDA driver installed on a system
can not be changed with Python's virtual enviroments.
% GPU builds\Fix{I don't understance GPU builds. do you mean
%   GPU vendor/version? give example} and specific CUDA builds. 
% Changing different virtual environments does not change the CUDA
% versions\Fix{Do you mean ``it is not possible to change the CUDA (or
%   GPU) version with Python's virtual environments''?} of the
% system. 
% That is why we use Docker when a bug relies on a specific
% version of CUDA to be installed in the system.
That is why we use Docker Containers with specific CUDA drivers to reproduce
such bugs instead of virtual enviroments.

\Fix{Please review and revise}

\subsection{Method}
\label{sec:method}

% \Fix{Abid, explain the steps you followed to create a bug
%   isntance. For example, (1) selected an issues (according to the
%   criteria defined in 2.1), (2) ...}

In order to add a bug to the dataset, a series of steps are followed.

\begin{itemize}
  \item A suitable issue is selected following the criteria defined in \ref{sec:selection-criterion}.
  \item A list of dependencies to reproduced the bug are written on a requirements text (requirements.txt).
  \item A python file is created that contains the bug revealing code, along with a test (pytest) that passes upon successfully trigerring the buggy behavior.
  \item If the bug depends on a CUDA-specific build, a Dockerfile is written that can create a docker container with the required CUDA-driver installed along with the requirements.txt file and the python script copied into it.
  \item A script is written that can create the virtual environment or the Docker container and execute the python test.
  \item All files are put into a single self-contained folder with the issue id in the name so that the tools exposed by the framework of our tool can find and execute the the bug reproduction script.
\end{itemize}

\Fix{Please review and revise}

\section{\tname: Database of Bugs in DL libraries}
\subsection{Bug Interface}

\begin{table}
  \centering
  \caption{\label{table:bug-interface}\tname's bug interface.}
\begin{tabular}{lp{6.5cm}}
\toprule
list-tests & List the tests available on this dataset\\
run-test & Runs one test\\
run-tests & Runs all the tests\\
show-info & Shows information about the tests available on this benchmark\\
stats & Shows statistics about this dataset (e.g., number of tests
that require GPU, number of tests that reproduce bugs in C or Python
code, etc.)\\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{table:bug-interface} shows \Fix{...}

\subsection{Test Oracle}

% \Fix{explain that the test passes if the bug can be reproduced}

The bug reproduction code includes python tests (pytest). The test oracle is satisfied when the bug is reproduced successfully. For bugs that throw an unexpected exception, the test catches the exception and reports the exception details along with a pass status. For bugs that result in incorrect output value, assertions are placed in the code to expect the incorrect output value, hence passing the test when the bug occurs. For bugs that crash after raising a signal, the code snippet is placed in a seperate file and executed, while the file contaning the actual test asserts the presence of the signal raised by the code. Upon successful reproduction, the original code crashes raising the proper signla and the assertion passes.

\Fix{Please review and revise}

\subsection{Usage and Setup}

\subsubsection{Running one bug instance}

% \Fix{walkthrough an example. Show input and part of output. Explain
%   the output.}

Let's take the bug from issue no. 120903 from \torch. To make sure the framework is accessible system wide, the "framework" folder in the tool needs to be added to the PATH environment. The command to run a specific test is "run-test". To reproduce this bug, the command line prompt is as the following:

\begin{lstlisting}[language=bash]
  $> run-test --library-name pytorch --bug-id 120903
\end{lstlisting}

The output will look like this upon reproducing successfully:

\begin{lstlisting}[language=bash]
  ...
  Versions of relevant libraries:
  [pip3] numpy==1.26.4
  [pip3] torch==2.2.0+cpu
  [conda] torch                     2.2.0+cpu                pypi_0    pypi
  ====== test session starts ======
  platform linux -- Python 3.10.0, pytest-8.2.0, pluggy-1.5.0
  rootdir: /home/mnaziri/Documents/DL_Testing/dnnbugs/pytorch/issue_120903
  collected 1 item                                                                            

  test_issue_120903.py Pytorch issue no. 120903
  Seed:  120903
  RuntimeError: !needs_dynamic_casting<func_t>::check(iter) INTERNAL ASSERT FAILED at "../aten/src/ATen/native/cpu/Loops.h":310, please report a bug to PyTorch. 
  ====== 1 passed in 0.96s ======
\end{lstlisting}

In this output, the dependencies used in the reproduction of the bug are listed. Then, the pytest is executed and the exception (bug) is caught. Afterwards, the issue id is printed and the exception that has been caught is printed to demonstrate the bug. At the end, the test is shown to be passed, signifying the successful reproduction of the bug.

\Fix{Please review and revise}

\subsubsection{Running FreeFuzz~\cite{wei2022free} on \tname.}

\Fix{walkthrough an example. Show input and part of output. Explain
  the output.}

\section{\tname: Use cases}

\subsection{Evaluating DL library testing techniques}
\subsection{Fault Detection}
\subsection{Program Repair}


\section{Related Work} 
Several bug datasets have been proposed in literature for general software. Some
popular examples includes software-artifact infrastructure repository
(SIR)~\cite{do2005supporting} which was one of the bug datasets containing 81
artificial faults in projects across multiple languages,
Defects4J~\cite{just2014defects4j} which includes 357 real bugs across five
large real-world Java projects, BugsInPy~\cite{widyasari2020bugsinpy} containing
493 real bugs from 17 real-world Python programs, and
BugsJS~\cite{vancsics2020relationship} containing 453 bugs across 10 Javascript
projects.
%
Google's FuzzBench~\cite{metzman2021fuzzbench} provides an evaluation platform
for comparing general purpose fuzzers. Magma~\cite{hazimeh2020magma} is another
fuzzing benchmark built by \emph{front-porting} real bugs in latest version of
projects. In contrast to these general purpose benchmarks, \tname includes
reproducible bugs for Deep Learning libraries like PyTorch and TensorFlow, and
allows systematic benchmarking for DLL fuzzers. 



% Defects4J\cite{just2014defects4j}
% BugsInPy~\cite{widyasari2020bugsinpy} Tests4Py~\cite{smytzek2024tests4py}
% Denchmark~\cite{kim2021denchmark}



\section{Conclusion and Future Work}

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

%%
%% If your work has an appendix, this is the place to put it.
\appendix

\section{Research Methods}


\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
